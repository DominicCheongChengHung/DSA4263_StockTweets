{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recreating the old model used for Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Stock                   Datetime user.screen_name  \\\n",
      "0       AXTG  2021-03-25 17:50:13+00:00          UCitnow   \n",
      "1       AXTG  2021-03-25 17:50:48+00:00          UCitnow   \n",
      "2       AXTG  2021-03-25 18:47:48+00:00           Rad7RR   \n",
      "3       AXTG  2021-03-25 20:02:03+00:00         GetScanz   \n",
      "4       AXTG  2021-03-25 20:02:48+00:00  christinebarnum   \n",
      "...      ...                        ...              ...   \n",
      "10071  EEENF  2021-04-09 01:00:55+00:00      TVTVentures   \n",
      "10072  EEENF  2021-04-09 01:06:47+00:00    BuyLowSell420   \n",
      "10073  EEENF  2021-04-09 01:06:56+00:00      superlars34   \n",
      "10074  EEENF  2021-04-09 01:07:55+00:00    DaveWhitman12   \n",
      "10075  EEENF  2021-04-09 01:14:21+00:00       jerocker79   \n",
      "\n",
      "                     id_str  \\\n",
      "0      1375142994920271872a   \n",
      "1      1375143141058080768a   \n",
      "2      1375157484063584261a   \n",
      "3      1375176172099747845a   \n",
      "4      1375176361560604679a   \n",
      "...                     ...   \n",
      "10071  1380324815689539585a   \n",
      "10072  1380326289735708672a   \n",
      "10073  1380326328524623873a   \n",
      "10074  1380326575414120448a   \n",
      "10075  1380328195778506753a   \n",
      "\n",
      "                                                    text  Sentiment  \\\n",
      "0                        #rt  rt    axtg new intern ceo           0   \n",
      "1                    rt  #rt  rt    axtg new intern ceo           0   \n",
      "2                 axtg big otc  gainerrocketrocketrocket          1   \n",
      "3      todays top penny stock gainers\\n\\nlttgf axtg s...          1   \n",
      "4      rt  todays top penny stock gainers\\n\\nlttgf ax...          1   \n",
      "...                                                  ...        ...   \n",
      "10071  holding until at least summer wdhr eeenf phil ...          1   \n",
      "10072   yep well now that the bushy ass side burned d...         -1   \n",
      "10073  vper rocket phil imtl abml uatg sfor ltnc intc...          1   \n",
      "10074  rt  eeenf moneybagmoneybagmoneybagmoneybagface...          1   \n",
      "10075   why eeenf to hcmc ive been waiting months for...         -1   \n",
      "\n",
      "       Known_Pumper  Price_Region  Inflection_Point  #solana  ...  today  \\\n",
      "0                 0           1.0               NaN      0.0  ...    0.0   \n",
      "1                 0           1.0               NaN      0.0  ...    0.0   \n",
      "2                 0           1.0               NaN      0.0  ...    0.0   \n",
      "3                 0           1.0               NaN      0.0  ...    0.0   \n",
      "4                 0           1.0               NaN      0.0  ...    0.0   \n",
      "...             ...           ...               ...      ...  ...    ...   \n",
      "10071             0           0.0               NaN      0.0  ...    0.0   \n",
      "10072             0           0.0               NaN      0.0  ...    0.0   \n",
      "10073             0           0.0               NaN      0.0  ...    0.0   \n",
      "10074             0           0.0               NaN      0.0  ...    0.0   \n",
      "10075             0           0.0               NaN      0.0  ...    0.0   \n",
      "\n",
      "       tomorrow  ttcm  update  vmhg  volume  wdlf  week  xrp  youre  \n",
      "0           0.0   0.0     0.0   0.0     0.0   0.0   0.0  0.0    0.0  \n",
      "1           0.0   0.0     0.0   0.0     0.0   0.0   0.0  0.0    0.0  \n",
      "2           0.0   0.0     0.0   0.0     0.0   0.0   0.0  0.0    0.0  \n",
      "3           0.0   0.0     0.0   0.0     0.0   0.0   0.0  0.0    0.0  \n",
      "4           0.0   0.0     0.0   0.0     0.0   0.0   0.0  0.0    0.0  \n",
      "...         ...   ...     ...   ...     ...   ...   ...  ...    ...  \n",
      "10071       0.0   0.0     0.0   0.0     0.0   0.0   0.0  0.0    0.0  \n",
      "10072       0.0   0.0     0.0   0.0     0.0   0.0   0.0  0.0    0.0  \n",
      "10073       0.0   0.0     0.0   0.0     0.0   0.0   0.0  0.0    0.0  \n",
      "10074       0.0   0.0     0.0   0.0     0.0   0.0   0.0  0.0    0.0  \n",
      "10075       0.0   0.0     0.0   0.0     0.0   0.0   0.0  0.0    0.0  \n",
      "\n",
      "[10076 rows x 109 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re\n",
    "import emoji\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Preprocesses the text by removing emojis, symbols, URLs, mentions, and punctuation.\n",
    "    \"\"\"\n",
    "    if isinstance(text, str): #check if text is a string.\n",
    "        text = emoji.demojize(text)  # Replace emojis with text descriptions\n",
    "        text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)  # Remove URLs\n",
    "        text = re.sub(r'@\\w+', '', text)  # Remove mentions\n",
    "        text = re.sub(r'[^\\w\\s#]', '', text)  # Remove punctuation (except hashtags)\n",
    "        text = re.sub(r'[^a-zA-Z\\s#]', '', text) #remove symbols.\n",
    "        text = text.lower()\n",
    "        return text\n",
    "    else:\n",
    "        return \"\" #if it is not a string, return empty string.\n",
    "\n",
    "def create_tfidf_features(df, text_column='text', max_features=100):\n",
    "    \"\"\"\n",
    "    Creates TF-IDF features from the specified text column with improved preprocessing.\n",
    "    \"\"\"\n",
    "\n",
    "    # Apply preprocessing to the text column\n",
    "    df[text_column] = df[text_column].apply(preprocess_text)\n",
    "\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        max_features=max_features,\n",
    "        stop_words='english',\n",
    "        token_pattern=r'\\b\\w+\\b|\\B#\\w+\\b'  # Include hashtags\n",
    "    )\n",
    "    tfidf_matrix = vectorizer.fit_transform(df[text_column])\n",
    "    tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "    df = pd.concat([df.reset_index(drop=True), tfidf_df.reset_index(drop=True)], axis=1)\n",
    "\n",
    "    return df,vectorizer\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"../data/scored_tweets_total.csv\")\n",
    "\n",
    "df_with_tfidf_features,vectorizer = create_tfidf_features(df.copy())\n",
    "\n",
    "print(df_with_tfidf_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../model_training/models/tfidf_vectorizer.joblib']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "vectorizer_path = \"../model_training/models/tfidf_vectorizer.joblib\"\n",
    "\n",
    "joblib.dump(vectorizer, vectorizer_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying it onto new dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tfidf_features_from_saved(df, vectorizer, text_column='Tweet'):\n",
    "    \"\"\"\n",
    "    Creates TF-IDF features using a pre-trained vectorizer.\n",
    "    \"\"\"\n",
    "    df[text_column] = df[text_column].apply(preprocess_text)\n",
    "\n",
    "    tfidf_matrix = vectorizer.transform(df[text_column])\n",
    "    tfidf_array = tfidf_matrix.toarray()  # Convert sparse matrix to dense array\n",
    "    tfidf_df = pd.DataFrame(tfidf_array, columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "    # Concatenate the original DataFrame with the TF-IDF features\n",
    "    df = pd.concat([df.reset_index(drop=True), tfidf_df.reset_index(drop=True)], axis=1)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        Date  \\\n",
      "0  2022-09-29 23:41:16+00:00   \n",
      "1  2022-09-29 23:24:43+00:00   \n",
      "2  2022-09-29 23:18:08+00:00   \n",
      "3  2022-09-29 22:40:07+00:00   \n",
      "4  2022-09-29 22:27:05+00:00   \n",
      "\n",
      "                                               Tweet Stock Name Company Name  \\\n",
      "0  mainstream media has done an amazing job at br...       TSLA  Tesla, Inc.   \n",
      "1  tesla delivery estimates are at around k from ...       TSLA  Tesla, Inc.   \n",
      "2   even if i include m unvested rsus as of  addi...       TSLA  Tesla, Inc.   \n",
      "3     hahaha why are you still trying to stop tes...       TSLA  Tesla, Inc.   \n",
      "4    stop trying to kill kids you sad deranged ol...       TSLA  Tesla, Inc.   \n",
      "\n",
      "   #solana  aabb  ada       amp  athdoubleexclamationmark  azfl  ...  \\\n",
      "0      0.0   0.0  0.0  0.835086                       0.0   0.0  ...   \n",
      "1      0.0   0.0  0.0  0.000000                       0.0   0.0  ...   \n",
      "2      0.0   0.0  0.0  0.000000                       0.0   0.0  ...   \n",
      "3      0.0   0.0  0.0  0.000000                       0.0   0.0  ...   \n",
      "4      0.0   0.0  0.0  0.000000                       0.0   0.0  ...   \n",
      "\n",
      "      today  tomorrow  ttcm  update  vmhg  volume  wdlf  week  xrp  youre  \n",
      "0  0.272974  0.000000   0.0     0.0   0.0     0.0   0.0   0.0  0.0    0.0  \n",
      "1  0.000000  0.000000   0.0     0.0   0.0     0.0   0.0   0.0  0.0    0.0  \n",
      "2  0.000000  0.368098   0.0     0.0   0.0     0.0   0.0   0.0  0.0    0.0  \n",
      "3  0.000000  0.000000   0.0     0.0   0.0     0.0   0.0   0.0  0.0    0.0  \n",
      "4  0.000000  0.000000   0.0     0.0   0.0     0.0   0.0   0.0  0.0    0.0  \n",
      "\n",
      "[5 rows x 104 columns]\n"
     ]
    }
   ],
   "source": [
    "loaded_vectorizer = joblib.load('../model_training/models/tfidf_vectorizer.joblib')\n",
    "\n",
    "# Load your new dataset\n",
    "new_df = pd.read_csv(\"../data/stock_tweets_test_on_real_data.csv\")\n",
    "\n",
    "# Create TF-IDF features for the new dataset using the loaded vectorizer\n",
    "new_df_with_tfidf = create_tfidf_features_from_saved(new_df.copy(), loaded_vectorizer) # Use .copy() to avoid modifying the original DataFrame\n",
    "\n",
    "# Now new_df_with_tfidf will have the original columns of new_df\n",
    "# plus the TF-IDF features as new columns.\n",
    "print(new_df_with_tfidf.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The new DataFrame with TF-IDF features has been saved to: ../data/tfidf_data_with_new_tweets.csv\n"
     ]
    }
   ],
   "source": [
    "# After creating the new_df_with_tfidf DataFrame:\n",
    "\n",
    "output_file_path = '../data/tfidf_data_with_new_tweets.csv'  # Choose your desired filename\n",
    "new_df_with_tfidf.to_csv(output_file_path, index=False)  # index=False prevents writing the DataFrame index to the CSV\n",
    "\n",
    "print(f\"The new DataFrame with TF-IDF features has been saved to: {output_file_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
