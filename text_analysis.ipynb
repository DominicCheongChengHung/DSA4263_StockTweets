{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-08T12:48:45.774808Z",
     "start_time": "2025-04-08T12:48:43.324401Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import emoji\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, roc_auc_score, confusion_matrix\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from imblearn.over_sampling import SMOTE"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-08T12:48:47.519680Z",
     "start_time": "2025-04-08T12:48:47.263309Z"
    }
   },
   "cell_type": "code",
   "source": [
    "normal_data = pd.read_csv(r\"C:\\Users\\limti\\PycharmProjects\\DSA4263_StockTweets\\data\\normal_data.csv\")\n",
    "tfidf_data = pd.read_csv(r\"C:\\Users\\limti\\PycharmProjects\\DSA4263_StockTweets\\data\\tfidf_data.csv\")"
   ],
   "id": "6217a42cdff1a963",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-08T12:48:48.154100Z",
     "start_time": "2025-04-08T12:48:48.039574Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def count_hashtags(text):\n",
    "    if isinstance(text, str):\n",
    "        return len(re.findall(r'#\\w+', text))\n",
    "    return 0\n",
    "\n",
    "def count_emojis(text):\n",
    "    if isinstance(text, str):\n",
    "        return len([char for char in text if char in emoji.EMOJI_DATA])\n",
    "    return 0\n",
    "\n",
    "\n",
    "# Apply the functions to your text column\n",
    "normal_data['hashtag_count'] = normal_data['text'].apply(count_hashtags) #replace text_column\n",
    "normal_data['emoji_count'] = normal_data['text'].apply(count_emojis) #replace text_column\n",
    "\n",
    "# Display the DataFrame with the new columns\n",
    "print(normal_data[['text', 'hashtag_count', 'emoji_count']].head())"
   ],
   "id": "5ca07db8bf55013",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  hashtag_count  \\\n",
      "0                    #rt  rt    axtg new intern ceo               1   \n",
      "1                rt  #rt  rt    axtg new intern ceo               1   \n",
      "2             axtg big otc  gainerrocketrocketrocket              0   \n",
      "3  todays top penny stock gainers\\r\\n\\r\\nlttgf ax...              1   \n",
      "4  rt  todays top penny stock gainers\\r\\n\\r\\nlttg...              3   \n",
      "\n",
      "   emoji_count  \n",
      "0            0  \n",
      "1            0  \n",
      "2            0  \n",
      "3            0  \n",
      "4            0  \n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-08T12:48:49.397930Z",
     "start_time": "2025-04-08T12:48:49.390421Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data = normal_data[[\"text\", \"Known_Pumper\"]]\n",
    "print(data.head())"
   ],
   "id": "40906b849852f024",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  Known_Pumper\n",
      "0                    #rt  rt    axtg new intern ceo              0\n",
      "1                rt  #rt  rt    axtg new intern ceo              0\n",
      "2             axtg big otc  gainerrocketrocketrocket             0\n",
      "3  todays top penny stock gainers\\r\\n\\r\\nlttgf ax...             0\n",
      "4  rt  todays top penny stock gainers\\r\\n\\r\\nlttg...             0\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-08T13:22:40.293596Z",
     "start_time": "2025-04-08T13:22:40.266310Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import emoji\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.feature_selection import SelectKBest, chi2  # Add feature selection\n",
    "\n",
    "def count_hashtags(text):\n",
    "    if isinstance(text, str):\n",
    "        return len(re.findall(r'#\\w+', text))\n",
    "    return 0\n",
    "\n",
    "def count_emojis(text):\n",
    "    if isinstance(text, str):\n",
    "        return len([char for char in text if char in emoji.EMOJI_DATA])\n",
    "    return 0\n",
    "\n",
    "def preprocess_text(text):\n",
    "    if isinstance(text, str):\n",
    "        text = emoji.demojize(text)\n",
    "        text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "        text = re.sub(r'@\\w+', '', text)\n",
    "        text = re.sub(r'[^\\w\\s#]', '', text)\n",
    "        text = re.sub(r'[^a-zA-Z\\s#]', '', text)\n",
    "        text = text.lower()\n",
    "        return text\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "def align_tfidf_columns(train_tfidf_df, test_tfidf_df):\n",
    "    \"\"\"Aligns TF-IDF columns between training and test DataFrames.\"\"\"\n",
    "    train_cols = set(train_tfidf_df.columns)\n",
    "    test_cols = set(test_tfidf_df.columns)\n",
    "\n",
    "    common_cols = list(train_cols & test_cols)\n",
    "    missing_train_cols = list(test_cols - train_cols)\n",
    "    missing_test_cols = list(train_cols - test_cols)\n",
    "\n",
    "    for col in missing_train_cols:\n",
    "        train_tfidf_df[col] = 0\n",
    "\n",
    "    for col in missing_test_cols:\n",
    "        test_tfidf_df[col] = 0\n",
    "\n",
    "    return train_tfidf_df[common_cols + missing_test_cols], test_tfidf_df[common_cols + missing_train_cols]\n",
    "\n",
    "def create_ml_pipeline_with_tfidf_oversampling(df, classifier, text_column='text', target_column='Known_Pumper', numerical_columns=[\"emoji_count\", \"hashtag_count\"], oversampling=True, max_tfidf_features=1500, param_grid=None): #reduced max_tfidf_features\n",
    "    \"\"\"\n",
    "    Creates a machine learning pipeline with TF-IDF, emoji/hashtag counts, and optional SMOTE oversampling.\n",
    "    Returns metrics for both train and test sets.\n",
    "    \"\"\"\n",
    "\n",
    "    df['hashtag_count'] = df[text_column].apply(count_hashtags)\n",
    "    df['emoji_count'] = df[text_column].apply(count_emojis)\n",
    "    df[text_column] = df[text_column].apply(preprocess_text)\n",
    "\n",
    "    X = df.drop(target_column, axis=1)\n",
    "    y = df[target_column]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    vectorizer = TfidfVectorizer(max_features=max_tfidf_features, stop_words='english', ngram_range=(1, 2)) #added ngram\n",
    "    train_tfidf_matrix = vectorizer.fit_transform(X_train[text_column])\n",
    "    test_tfidf_matrix = vectorizer.transform(X_test[text_column])\n",
    "\n",
    "    train_tfidf_df = pd.DataFrame(train_tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "    test_tfidf_df = pd.DataFrame(test_tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "    train_tfidf_df, test_tfidf_df = align_tfidf_columns(train_tfidf_df, test_tfidf_df)\n",
    "\n",
    "    X_train = X_train.reset_index(drop=True)\n",
    "    train_tfidf_df = train_tfidf_df.reset_index(drop=True)\n",
    "    X_test = X_test.reset_index(drop=True)\n",
    "    test_tfidf_df = test_tfidf_df.reset_index(drop=True)\n",
    "    \n",
    "    X_train = pd.concat([X_train.drop(text_column, axis=1), train_tfidf_df], axis=1)\n",
    "    X_test = pd.concat([X_test.drop(text_column, axis=1), test_tfidf_df], axis=1)\n",
    "\n",
    "\n",
    "    tfidf_columns = [col for col in X_train.columns if col not in numerical_columns and col != 'hashtag_count' and col != 'emoji_count']\n",
    "\n",
    "    # Feature selection\n",
    "    feature_selector = SelectKBest(chi2, k=min(200, len(tfidf_columns))) #added feature selection\n",
    "    X_train_selected = feature_selector.fit_transform(X_train[tfidf_columns], y_train)\n",
    "    X_test_selected = feature_selector.transform(X_test[tfidf_columns])\n",
    "\n",
    "    selected_tfidf_columns = [tfidf_columns[i] for i in feature_selector.get_support(indices=True)]\n",
    "    X_train_selected_df = pd.DataFrame(X_train_selected, columns=selected_tfidf_columns)\n",
    "    X_test_selected_df = pd.DataFrame(X_test_selected, columns=selected_tfidf_columns)\n",
    "\n",
    "    X_train_final = pd.concat([X_train[numerical_columns].reset_index(drop=True), X_train_selected_df.reset_index(drop=True)], axis=1)\n",
    "    X_test_final = pd.concat([X_test[numerical_columns].reset_index(drop=True), X_test_selected_df.reset_index(drop=True)], axis=1)\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('numerical', MinMaxScaler(), numerical_columns + ['hashtag_count', 'emoji_count']),\n",
    "            ('tfidf', 'passthrough', selected_tfidf_columns) #use selected columns\n",
    "        ],\n",
    "        remainder='drop'\n",
    "    )\n",
    "\n",
    "    pipeline = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', classifier)\n",
    "    ])\n",
    "\n",
    "    if param_grid is not None:\n",
    "        grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='f1', verbose=2, n_jobs=3)\n",
    "        grid_search.fit(X_train_final, y_train)\n",
    "        pipeline = grid_search.best_estimator_\n",
    "\n",
    "    else:\n",
    "        pipeline.fit(X_train_final, y_train)\n",
    "\n",
    "    y_train_pred = pipeline.predict(X_train_final)\n",
    "    y_test_pred = pipeline.predict(X_test_final)\n",
    "\n",
    "    train_metrics = {\n",
    "        'accuracy': accuracy_score(y_train, y_train_pred),\n",
    "        'precision': precision_score(y_train, y_train_pred),\n",
    "        'recall': recall_score(y_train, y_train_pred),\n",
    "        'f1': f1_score(y_train, y_train_pred),\n",
    "        'roc_auc': roc_auc_score(y_train, pipeline.predict_proba(X_train_final)[:, 1]),\n",
    "        'confusion_matrix': confusion_matrix(y_train, y_train_pred)\n",
    "    }\n",
    "\n",
    "    test_metrics = {\n",
    "        'accuracy': accuracy_score(y_test, y_test_pred),\n",
    "        'precision': precision_score(y_test, y_test_pred),\n",
    "        'recall': recall_score(y_test, y_test_pred),\n",
    "        'f1': f1_score(y_test, y_test_pred),\n",
    "        'roc_auc': roc_auc_score(y_test, pipeline.predict_proba(X_test_final)[:, 1]),\n",
    "        'confusion_matrix': confusion_matrix(y_test, y_test_pred)\n",
    "    }\n",
    "    \n",
    "    return pipeline, vectorizer, train_metrics, test_metrics"
   ],
   "id": "a55f3f8fe5862995",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-08T13:22:40.995807Z",
     "start_time": "2025-04-08T13:22:40.986872Z"
    }
   },
   "cell_type": "code",
   "source": "data.columns",
   "id": "5b0bfbc783c664cd",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['text', 'Known_Pumper', 'hashtag_count', 'emoji_count'], dtype='object')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-08T13:25:13.952676Z",
     "start_time": "2025-04-08T13:22:41.339969Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "param_grid_logistic = {\n",
    "    'classifier__penalty': ['l1', 'l2'],  # Regularization type\n",
    "    'classifier__C': [0.001, 0.01, 0.1, 1, 10, 100],  # Inverse of regularization strength\n",
    "    'classifier__solver': ['liblinear', 'saga'],  # Solver algorithm\n",
    "    'classifier__class_weight': [None, 'balanced'] #Handle Imbalance\n",
    "}\n",
    "\n",
    "pipeline, vectorizer, train_metrics, test_metrics = create_ml_pipeline_with_tfidf_oversampling(data, classifier = LogisticRegression(), param_grid = param_grid_logistic)"
   ],
   "id": "25df2ca8b2193401",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\limti\\AppData\\Local\\Temp\\ipykernel_704\\3511989821.py:60: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['hashtag_count'] = df[text_column].apply(count_hashtags)\n",
      "C:\\Users\\limti\\AppData\\Local\\Temp\\ipykernel_704\\3511989821.py:61: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['emoji_count'] = df[text_column].apply(count_emojis)\n",
      "C:\\Users\\limti\\AppData\\Local\\Temp\\ipykernel_704\\3511989821.py:62: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[text_column] = df[text_column].apply(preprocess_text)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-08T13:25:13.997382Z",
     "start_time": "2025-04-08T13:25:13.965680Z"
    }
   },
   "cell_type": "code",
   "source": "train_metrics",
   "id": "26cf9b734563a642",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.8665594855305466,\n",
       " 'precision': 0.8740157480314961,\n",
       " 'recall': 0.8565916398713826,\n",
       " 'f1': 0.8652159792140305,\n",
       " 'roc_auc': np.float64(0.9445279928867567),\n",
       " 'confusion_matrix': array([[6815,  960],\n",
       "        [1115, 6660]])}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-08T13:25:14.008Z",
     "start_time": "2025-04-08T13:25:13.999387Z"
    }
   },
   "cell_type": "code",
   "source": "test_metrics",
   "id": "c442af38d52df816",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.8571428571428571,\n",
       " 'precision': 0.1461794019933555,\n",
       " 'recall': 0.5866666666666667,\n",
       " 'f1': 0.23404255319148937,\n",
       " 'roc_auc': np.float64(0.8131616005495448),\n",
       " 'confusion_matrix': array([[1684,  257],\n",
       "        [  31,   44]])}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-08T13:25:14.038281Z",
     "start_time": "2025-04-08T13:25:14.026078Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import pickle\n",
    "pipeline_path = os.path.join(r\"C:\\Users\\limti\\PycharmProjects\\DSA4263_StockTweets\\model_training\\tfidf_training\", \"lr_pipeline.pkl\")\n",
    "vectorizer_path = os.path.join(r\"C:\\Users\\limti\\PycharmProjects\\DSA4263_StockTweets\\model_training\\tfidf_training\", \"lr_vectorizer.pkl\")\n",
    "\n",
    "with open(pipeline_path, 'wb') as p:\n",
    "    pickle.dump(pipeline, p)\n",
    "\n",
    "with open(vectorizer_path, 'wb') as v:\n",
    "    pickle.dump(vectorizer, v)"
   ],
   "id": "5bb66abc373d3bf5",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-08T13:33:05.420352Z",
     "start_time": "2025-04-08T13:25:14.040290Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "param_grid_gradient_boosting = {\n",
    "    'classifier__n_estimators': [100, 200],  # Reduced number of estimators\n",
    "    'classifier__learning_rate': [0.05, 0.1],  # Reduced learning rate options\n",
    "    'classifier__max_depth': [3, 4],  # Reduced depth options\n",
    "    'classifier__subsample': [0.9, 1.0],  # Reduced subsample options\n",
    "    'classifier__max_features': ['sqrt', None] # Reduced number of features to consider.\n",
    "}\n",
    "\n",
    "\n",
    "pipeline, vectorizer, train_metrics, test_metrics = create_ml_pipeline_with_tfidf_oversampling(data, classifier = GradientBoostingClassifier(), param_grid=param_grid_gradient_boosting)"
   ],
   "id": "1155096e8e46bc0e",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\limti\\AppData\\Local\\Temp\\ipykernel_704\\3511989821.py:60: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['hashtag_count'] = df[text_column].apply(count_hashtags)\n",
      "C:\\Users\\limti\\AppData\\Local\\Temp\\ipykernel_704\\3511989821.py:61: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['emoji_count'] = df[text_column].apply(count_emojis)\n",
      "C:\\Users\\limti\\AppData\\Local\\Temp\\ipykernel_704\\3511989821.py:62: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[text_column] = df[text_column].apply(preprocess_text)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 32 candidates, totalling 160 fits\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-08T13:33:05.428788Z",
     "start_time": "2025-04-08T13:33:05.422567Z"
    }
   },
   "cell_type": "code",
   "source": "train_metrics",
   "id": "823319ad33d1bbcd",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.934983922829582,\n",
       " 'precision': 0.9341463414634147,\n",
       " 'recall': 0.9359485530546624,\n",
       " 'f1': 0.9350465788628333,\n",
       " 'roc_auc': np.float64(0.9809925455692147),\n",
       " 'confusion_matrix': array([[7262,  513],\n",
       "        [ 498, 7277]])}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-08T13:33:05.437665Z",
     "start_time": "2025-04-08T13:33:05.431792Z"
    }
   },
   "cell_type": "code",
   "source": "test_metrics",
   "id": "7a806c06239363f5",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.9047619047619048,\n",
       " 'precision': 0.193717277486911,\n",
       " 'recall': 0.49333333333333335,\n",
       " 'f1': 0.2781954887218045,\n",
       " 'roc_auc': np.float64(0.8406113687102867),\n",
       " 'confusion_matrix': array([[1787,  154],\n",
       "        [  38,   37]])}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-08T13:33:05.464820Z",
     "start_time": "2025-04-08T13:33:05.438669Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import pickle\n",
    "pipeline_path = os.path.join(r\"C:\\Users\\limti\\PycharmProjects\\DSA4263_StockTweets\\model_training\\tfidf_training\", \"gb_pipeline.pkl\")\n",
    "vectorizer_path = os.path.join(r\"C:\\Users\\limti\\PycharmProjects\\DSA4263_StockTweets\\model_training\\tfidf_training\", \"gb_vectorizer.pkl\")\n",
    "\n",
    "with open(pipeline_path, 'wb') as p:\n",
    "    pickle.dump(pipeline, p)\n",
    "\n",
    "with open(vectorizer_path, 'wb') as v:\n",
    "    pickle.dump(vectorizer, v)"
   ],
   "id": "68651b1e30ab401d",
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-08T13:33:05.474437Z",
     "start_time": "2025-04-08T13:33:05.467823Z"
    }
   },
   "cell_type": "code",
   "source": "data[\"Known_Pumper\"]",
   "id": "32e43ffb42c881c9",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        0\n",
       "1        0\n",
       "2        0\n",
       "3        0\n",
       "4        0\n",
       "        ..\n",
       "10071    0\n",
       "10072    0\n",
       "10073    0\n",
       "10074    0\n",
       "10075    0\n",
       "Name: Known_Pumper, Length: 10076, dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "from sklearn.ensemble import RandomFor",
   "id": "683ceec84240a153"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
