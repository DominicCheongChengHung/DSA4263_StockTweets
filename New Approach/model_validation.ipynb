{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-13T11:36:47.116763Z",
     "start_time": "2025-04-13T11:36:46.561796Z"
    }
   },
   "source": [
    "from tf_idf_new import PumpDetection\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# This python notebook is to test the model on another dataset of tweets\n",
    "\n",
    "We will be using the stock market dataset to detect any price anomalies (detecting potential pump using Isolation Forest) and test the model\n",
    "\n",
    "We're detecting pump-and-dump schemes on Twitter by correlating NLP model predictions of manipulative tweets with stock market price anomalies identified via Isolation Forest. This analysis evaluates the model's effectiveness in capturing these anomalies, assesses false positive/negative rates, analyzes time-based patterns, and identifies key features and user behaviors associated with these schemes. We're aiming to quantify the model's impact on detecting market manipulation by comparing predicted pump-and-dump tweets to significant price fluctuations, while acknowledging potential challenges like causality vs. correlation, time lags, and data noise."
   ],
   "id": "1d0e56e274315d9d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-13T11:36:47.323788Z",
     "start_time": "2025-04-13T11:36:47.117744Z"
    }
   },
   "cell_type": "code",
   "source": [
    "test_data = pd.read_csv(r\"../data/stock_tweets_test_on_real_data.csv\")\n",
    "stock_data = pd.read_csv(r\"../data/stock_yfinance_data_test_on_real_data.csv\")"
   ],
   "id": "782580ce8761b8f3",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Detecting Price Anomalies",
   "id": "39231d767ebc35f3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-13T11:36:59.092677Z",
     "start_time": "2025-04-13T11:36:58.914834Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "def detect_pump_and_dump(stock_prices, tweets, lags=[1, 3, 5]):\n",
    "    \"\"\"Detects potential pump-and-dump schemes.\"\"\"\n",
    "\n",
    "    stock_prices['Date'] = pd.to_datetime(stock_prices['Date']) #convert to datetime\n",
    "    tweets['Date'] = pd.to_datetime(pd.to_datetime(tweets['Date']).dt.date) #extract only the date\n",
    "\n",
    "    for lag in lags:\n",
    "        stock_prices[f'price_change_{lag}'] = stock_prices['Close'].pct_change(periods=lag) #Capital C\n",
    "        stock_prices[f'volume_change_{lag}'] = stock_prices['Volume'].pct_change(periods=lag) #Capital V\n",
    "\n",
    "    # Use all price and volume change columns for anomaly detection\n",
    "    price_volume_features = [col for col in stock_prices.columns if 'change' in col]\n",
    "\n",
    "    stock_prices['price_anomaly'] = IsolationForest().fit_predict(stock_prices[price_volume_features])\n",
    "\n",
    "    merged_data = pd.merge(stock_prices, tweets, on=['Date', 'Stock Name'], how='inner')\n",
    "\n",
    "\n",
    "    return merged_data\n",
    "\n",
    "pump_dump_data = detect_pump_and_dump(stock_data, test_data, lags=[1, 3, 5])\n",
    "pump_dump_data['price_anomaly'] = (pump_dump_data['price_anomaly'] == -1).astype(int)"
   ],
   "id": "7cb264aced4023da",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-13T11:37:02.236313Z",
     "start_time": "2025-04-13T11:37:02.230316Z"
    }
   },
   "cell_type": "code",
   "source": "pump_dump_data[\"price_anomaly\"].value_counts()",
   "id": "cfc9e556edffcb0a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "price_anomaly\n",
       "0    57901\n",
       "1     5775\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## NLP Model Prediction",
   "id": "9d70eab5f000aa5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-13T11:37:03.609047Z",
     "start_time": "2025-04-13T11:37:03.577941Z"
    }
   },
   "cell_type": "code",
   "source": [
    "rf_path = [\n",
    "    r\"model/random_forest_classification_pipeline.joblib\",\n",
    "    r\"model/random_forest_classification_vectorizer.joblib\"\n",
    "]\n",
    "\n",
    "model = PumpDetection(rf_path[0], rf_path[1])"
   ],
   "id": "28b442136518b2a1",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-13T11:44:34.256666Z",
     "start_time": "2025-04-13T11:37:04.183488Z"
    }
   },
   "cell_type": "code",
   "source": "pump_dump_data[\"model_prediction\"] = model.predict(pump_dump_data[\"Tweet\"])",
   "id": "6753f155fc14852",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translation Error: No features in text.\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-13T11:47:57.071605Z",
     "start_time": "2025-04-13T11:47:57.067035Z"
    }
   },
   "cell_type": "code",
   "source": "pump_dump_data[\"model_prediction\"].value_counts()",
   "id": "581630baf360a0ce",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model_prediction\n",
       "0    35653\n",
       "1    28023\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-13T11:59:20.962907Z",
     "start_time": "2025-04-13T11:59:20.956306Z"
    }
   },
   "cell_type": "code",
   "source": [
    "total_price_anomaly = pump_dump_data[\"price_anomaly\"].sum()\n",
    "predicted_pump_tweet = pump_dump_data[pump_dump_data[\"model_prediction\"] == 1]\n",
    "number_of_predicted_pump_tweet_resulted_in_price_anomaly = predicted_pump_tweet[\"price_anomaly\"].sum()\n",
    "\n",
    "percentage_captured = (number_of_predicted_pump_tweet_resulted_in_price_anomaly / total_price_anomaly) * 100\n",
    "print(f\"Total Price Anomaly: {total_price_anomaly}\")\n",
    "print(f\"Price Anomaly Captured by Predicted Pump Tweets: {number_of_predicted_pump_tweet_resulted_in_price_anomaly}\")\n",
    "print(f\"Percentage of Price Anomaly Captured: {percentage_captured:.2f}%\")"
   ],
   "id": "5290714824bbcf5e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Price Anomaly: 5775\n",
      "Price Anomaly Captured by Predicted Pump Tweets: 2521\n",
      "Percentage of Price Anomaly Captured: 43.65%\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-13T11:59:29.061052Z",
     "start_time": "2025-04-13T11:59:29.054196Z"
    }
   },
   "cell_type": "code",
   "source": [
    "false_positives = len(pump_dump_data[(pump_dump_data[\"model_prediction\"] == 1) & (pump_dump_data[\"price_anomaly\"] == 0)])\n",
    "false_negatives = len(pump_dump_data[(pump_dump_data[\"model_prediction\"] == 0) & (pump_dump_data[\"price_anomaly\"] == 1)])\n",
    "\n",
    "print(f\"False Positives: {false_positives}\")\n",
    "print(f\"False Negatives: {false_negatives}\")"
   ],
   "id": "408b227a189ed5c8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False Positives: 25502\n",
      "False Negatives: 3254\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-13T11:59:38.338582Z",
     "start_time": "2025-04-13T11:59:37.697565Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming 'timestamp' column exists\n",
    "pump_dump_data.plot(x=\"timestamp\", y=\"price_anomaly\", figsize=(12, 6))\n",
    "plt.title(\"Price Anomaly Over Time\")\n",
    "plt.xlabel(\"Timestamp\")\n",
    "plt.ylabel(\"Price Anomaly\")\n",
    "plt.show()"
   ],
   "id": "4e7de40951bbeed5",
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'timestamp'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyError\u001B[39m                                  Traceback (most recent call last)",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\DSA4263_StockTweets\\.venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001B[39m, in \u001B[36mIndex.get_loc\u001B[39m\u001B[34m(self, key)\u001B[39m\n\u001B[32m   3804\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m3805\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_engine\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget_loc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcasted_key\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   3806\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err:\n",
      "\u001B[36mFile \u001B[39m\u001B[32mindex.pyx:167\u001B[39m, in \u001B[36mpandas._libs.index.IndexEngine.get_loc\u001B[39m\u001B[34m()\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mindex.pyx:196\u001B[39m, in \u001B[36mpandas._libs.index.IndexEngine.get_loc\u001B[39m\u001B[34m()\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001B[39m, in \u001B[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001B[39m\u001B[34m()\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001B[39m, in \u001B[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001B[39m\u001B[34m()\u001B[39m\n",
      "\u001B[31mKeyError\u001B[39m: 'timestamp'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[31mKeyError\u001B[39m                                  Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[14]\u001B[39m\u001B[32m, line 4\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mmatplotlib\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mpyplot\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mplt\u001B[39;00m\n\u001B[32m      3\u001B[39m \u001B[38;5;66;03m# Assuming 'timestamp' column exists\u001B[39;00m\n\u001B[32m----> \u001B[39m\u001B[32m4\u001B[39m \u001B[43mpump_dump_data\u001B[49m\u001B[43m.\u001B[49m\u001B[43mplot\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mtimestamp\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mprice_anomaly\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfigsize\u001B[49m\u001B[43m=\u001B[49m\u001B[43m(\u001B[49m\u001B[32;43m12\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[32;43m6\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m      5\u001B[39m plt.title(\u001B[33m\"\u001B[39m\u001B[33mPrice Anomaly Over Time\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m      6\u001B[39m plt.xlabel(\u001B[33m\"\u001B[39m\u001B[33mTimestamp\u001B[39m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\DSA4263_StockTweets\\.venv\\Lib\\site-packages\\pandas\\plotting\\_core.py:995\u001B[39m, in \u001B[36mPlotAccessor.__call__\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m    993\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m is_integer(x) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m data.columns._holds_integer():\n\u001B[32m    994\u001B[39m     x = data_cols[x]\n\u001B[32m--> \u001B[39m\u001B[32m995\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\u001B[43mdata\u001B[49m\u001B[43m[\u001B[49m\u001B[43mx\u001B[49m\u001B[43m]\u001B[49m, ABCSeries):\n\u001B[32m    996\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[33m\"\u001B[39m\u001B[33mx must be a label or position\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m    997\u001B[39m data = data.set_index(x)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\DSA4263_StockTweets\\.venv\\Lib\\site-packages\\pandas\\core\\frame.py:4102\u001B[39m, in \u001B[36mDataFrame.__getitem__\u001B[39m\u001B[34m(self, key)\u001B[39m\n\u001B[32m   4100\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.columns.nlevels > \u001B[32m1\u001B[39m:\n\u001B[32m   4101\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._getitem_multilevel(key)\n\u001B[32m-> \u001B[39m\u001B[32m4102\u001B[39m indexer = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mcolumns\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget_loc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   4103\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m is_integer(indexer):\n\u001B[32m   4104\u001B[39m     indexer = [indexer]\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\DSA4263_StockTweets\\.venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001B[39m, in \u001B[36mIndex.get_loc\u001B[39m\u001B[34m(self, key)\u001B[39m\n\u001B[32m   3807\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(casted_key, \u001B[38;5;28mslice\u001B[39m) \u001B[38;5;129;01mor\u001B[39;00m (\n\u001B[32m   3808\u001B[39m         \u001B[38;5;28misinstance\u001B[39m(casted_key, abc.Iterable)\n\u001B[32m   3809\u001B[39m         \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28many\u001B[39m(\u001B[38;5;28misinstance\u001B[39m(x, \u001B[38;5;28mslice\u001B[39m) \u001B[38;5;28;01mfor\u001B[39;00m x \u001B[38;5;129;01min\u001B[39;00m casted_key)\n\u001B[32m   3810\u001B[39m     ):\n\u001B[32m   3811\u001B[39m         \u001B[38;5;28;01mraise\u001B[39;00m InvalidIndexError(key)\n\u001B[32m-> \u001B[39m\u001B[32m3812\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m(key) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01merr\u001B[39;00m\n\u001B[32m   3813\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m:\n\u001B[32m   3814\u001B[39m     \u001B[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001B[39;00m\n\u001B[32m   3815\u001B[39m     \u001B[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001B[39;00m\n\u001B[32m   3816\u001B[39m     \u001B[38;5;66;03m#  the TypeError.\u001B[39;00m\n\u001B[32m   3817\u001B[39m     \u001B[38;5;28mself\u001B[39m._check_indexing_error(key)\n",
      "\u001B[31mKeyError\u001B[39m: 'timestamp'"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-13T12:00:11.794859Z",
     "start_time": "2025-04-13T12:00:11.789590Z"
    }
   },
   "cell_type": "code",
   "source": [
    "correlation_matrix = pump_dump_data[[\"price_anomaly\", \"model_prediction\"]].corr()\n",
    "print(correlation_matrix)"
   ],
   "id": "6a9bccd92c1eccb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  price_anomaly  model_prediction\n",
      "price_anomaly          1.000000         -0.002259\n",
      "model_prediction      -0.002259          1.000000\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-13T12:00:54.177008Z",
     "start_time": "2025-04-13T12:00:54.147789Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "precision = precision_score(pump_dump_data[\"price_anomaly\"], pump_dump_data[\"model_prediction\"])\n",
    "recall = recall_score(pump_dump_data[\"price_anomaly\"], pump_dump_data[\"model_prediction\"])\n",
    "f1 = f1_score(pump_dump_data[\"price_anomaly\"], pump_dump_data[\"model_prediction\"])\n",
    "roc_auc = roc_auc_score(pump_dump_data[\"price_anomaly\"], pump_dump_data[\"model_prediction\"])\n",
    "\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"F1-score: {f1:.2f}\")\n",
    "print(f\"ROC AUC: {roc_auc:.2f}\")"
   ],
   "id": "3d161d5a6f294fe6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.09\n",
      "Recall: 0.44\n",
      "F1-score: 0.15\n",
      "ROC AUC: 0.50\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Your model evaluation strategy centers on correlating NLP-detected pump-and-dump tweets with significant stock market price anomalies, identified through Isolation Forest. This method moves beyond standard NLP metrics by directly linking tweet predictions to real-world market impact. We quantify the model's success by measuring the percentage of total price anomalies captured by its predictions, effectively assessing its ability to identify tweets that coincide with manipulative market activity.\n",
    "\n",
    "Furthermore, we analyze false positives and false negatives to understand the model's precision and recall in this context. Time-based and user-based patterns are examined to provide deeper insights into when and where the model performs best, and which users are most associated with potential manipulation. Standard classification metrics like precision, recall, F1-score, and ROC AUC are also employed to provide a comprehensive evaluation.\n",
    "\n",
    "This approach evaluates the models by assessing their ability to predict tweets that directly correlate with unusual market behavior, a strong indicator of manipulation. This provides a more tangible measure of the model's effectiveness in detecting pump-and-dump schemes compared to traditional NLP metrics, as it focuses on the real-world financial impact of the predicted tweets."
   ],
   "id": "4cd6014660295445"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "d30a1ea846623c04"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
