{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data/scored_tweets_final_translated.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading VADER lexicon...\n",
      "VADER lexicon downloaded successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/dominiccheong/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Stock                   Datetime user.screen_name  \\\n",
      "0       AXTG  2021-03-25 17:50:13+00:00          UCitnow   \n",
      "1       AXTG  2021-03-25 17:50:48+00:00          UCitnow   \n",
      "2       AXTG  2021-03-25 18:47:48+00:00           Rad7RR   \n",
      "3       AXTG  2021-03-25 20:02:03+00:00         GetScanz   \n",
      "4       AXTG  2021-03-25 20:02:48+00:00  christinebarnum   \n",
      "...      ...                        ...              ...   \n",
      "10071  EEENF  2021-04-09 01:00:55+00:00      TVTVentures   \n",
      "10072  EEENF  2021-04-09 01:06:47+00:00    BuyLowSell420   \n",
      "10073  EEENF  2021-04-09 01:06:56+00:00      superlars34   \n",
      "10074  EEENF  2021-04-09 01:07:55+00:00    DaveWhitman12   \n",
      "10075  EEENF  2021-04-09 01:14:21+00:00       jerocker79   \n",
      "\n",
      "                     id_str  \\\n",
      "0      1375142994920271872a   \n",
      "1      1375143141058080768a   \n",
      "2      1375157484063584261a   \n",
      "3      1375176172099747845a   \n",
      "4      1375176361560604679a   \n",
      "...                     ...   \n",
      "10071  1380324815689539585a   \n",
      "10072  1380326289735708672a   \n",
      "10073  1380326328524623873a   \n",
      "10074  1380326575414120448a   \n",
      "10075  1380328195778506753a   \n",
      "\n",
      "                                                    text  Sentiment  \\\n",
      "0      #RT @ldev117: RT @UCitnow: @ShortSqueezed1 @EV...          0   \n",
      "1      RT @UCitnow: #RT @ldev117: RT @UCitnow: @Short...          0   \n",
      "2                                $AXTG Big OTC % Gainer!          1   \n",
      "3      Today’s Top Penny Stock Gainers\\n\\n$LTTGF $AXT...          1   \n",
      "4      RT @GetScanz: Today’s Top Penny Stock Gainers\\...          1   \n",
      "...                                                  ...        ...   \n",
      "10071  Holding until at least summer $WDHR $EEENF $PH...          1   \n",
      "10072  @TheFlyingScotto Yep well now that the bushy a...         -1   \n",
      "10073  $VPER  $PHIL $IMTL $ABML $UATG $SFOR $LTNC $IN...          1   \n",
      "10074                          RT @Jasontrade99: $ eeenf          1   \n",
      "10075  @OTCAmateur Why $EEENF to $HCMC? I’ve been wai...         -1   \n",
      "\n",
      "       Known_Pumper  Price_Region  Inflection_Point  user_pumper_probability  \\\n",
      "0                 0           1.0               NaN                 0.006720   \n",
      "1                 0           1.0               NaN                 0.006720   \n",
      "2                 0           1.0               NaN                 0.025278   \n",
      "3                 0           1.0               NaN                 0.025278   \n",
      "4                 0           1.0               NaN                 0.025278   \n",
      "...             ...           ...               ...                      ...   \n",
      "10071             0           0.0               NaN                 0.025278   \n",
      "10072             0           0.0               NaN                 0.025278   \n",
      "10073             0           0.0               NaN                 0.025278   \n",
      "10074             0           0.0               NaN                 0.025278   \n",
      "10075             0           0.0               NaN                 0.025278   \n",
      "\n",
      "       hashtag_count  emoji_count  keyword_count  stock_ticker_count  \\\n",
      "0                  1            0              0                   1   \n",
      "1                  1            0              0                   1   \n",
      "2                  0            3              0                   1   \n",
      "3                  1            0              1                   9   \n",
      "4                  3            0              1                   9   \n",
      "...              ...          ...            ...                 ...   \n",
      "10071              0            4              0                   5   \n",
      "10072              0            0              0                   0   \n",
      "10073              0            1              0                  15   \n",
      "10074              0           17              0                   1   \n",
      "10075              0            0              0                   2   \n",
      "\n",
      "       url_count  pumper_category  sentiment_score sentiment_category  \n",
      "0              1                0           0.0000            neutral  \n",
      "1              1                0           0.0000            neutral  \n",
      "2              0                0           0.0000            neutral  \n",
      "3              1                0           0.2023           positive  \n",
      "4              0                0           0.2023           positive  \n",
      "...          ...              ...              ...                ...  \n",
      "10071          0                0           0.3802           positive  \n",
      "10072          1                0          -0.0516           negative  \n",
      "10073          1                0           0.0772           positive  \n",
      "10074          0                0           0.0000            neutral  \n",
      "10075          0                0          -0.4588           negative  \n",
      "\n",
      "[10076 rows x 18 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "def ensure_vader_lexicon():\n",
    "    \"\"\"Ensures the VADER lexicon is downloaded.\"\"\"\n",
    "    try:\n",
    "        nltk.data.find('vader_lexicon')\n",
    "    except LookupError:\n",
    "        print(\"Downloading VADER lexicon...\")\n",
    "        nltk.download('vader_lexicon')\n",
    "        print(\"VADER lexicon downloaded successfully.\")\n",
    "\n",
    "ensure_vader_lexicon() # download the lexicon\n",
    "\n",
    "# Initialize SentimentIntensityAnalyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "def get_sentiment_score(text):\n",
    "    \"\"\"Calculates the compound sentiment score for a given text.\"\"\"\n",
    "    if isinstance(text, str):  # Handle NaN and other non-string values\n",
    "        vs = analyzer.polarity_scores(text)\n",
    "        return vs['compound']\n",
    "    else:\n",
    "        return 0  # Or any default value you prefer\n",
    "\n",
    "def categorize_sentiment(score):\n",
    "    \"\"\"Categorizes sentiment score into positive, negative, or neutral.\"\"\"\n",
    "    if score >= 0.05:\n",
    "        return 'positive'\n",
    "    elif score <= -0.05:\n",
    "        return 'negative'\n",
    "    else:\n",
    "        return 'neutral'\n",
    "\n",
    "def analyze_sentiment_dataframe(df, text_column='text'):\n",
    "    \"\"\"Iterates through a DataFrame and adds sentiment scores and categories.\"\"\"\n",
    "\n",
    "    # Ensure the provided column exists\n",
    "    if text_column not in df.columns:\n",
    "        raise ValueError(f\"Column '{text_column}' not found in DataFrame.\")\n",
    "\n",
    "    sentiment_scores = []\n",
    "    sentiment_categories = []\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        text = row[text_column]\n",
    "        score = get_sentiment_score(text)\n",
    "        sentiment_scores.append(score)\n",
    "        sentiment_categories.append(categorize_sentiment(score))\n",
    "\n",
    "    df['sentiment_score'] = sentiment_scores #numerical sentiment\n",
    "    df['sentiment_category'] = sentiment_categories #categorical sentiment\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "df = analyze_sentiment_dataframe(data, text_column='text')  # Use your actual DataFrame and column name\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Stock                   Datetime user.screen_name  \\\n",
      "0       AXTG  2021-03-25 17:50:13+00:00          UCitnow   \n",
      "1       AXTG  2021-03-25 17:50:48+00:00          UCitnow   \n",
      "2       AXTG  2021-03-25 18:47:48+00:00           Rad7RR   \n",
      "3       AXTG  2021-03-25 20:02:03+00:00         GetScanz   \n",
      "4       AXTG  2021-03-25 20:02:48+00:00  christinebarnum   \n",
      "...      ...                        ...              ...   \n",
      "10071  EEENF  2021-04-09 01:00:55+00:00      TVTVentures   \n",
      "10072  EEENF  2021-04-09 01:06:47+00:00    BuyLowSell420   \n",
      "10073  EEENF  2021-04-09 01:06:56+00:00      superlars34   \n",
      "10074  EEENF  2021-04-09 01:07:55+00:00    DaveWhitman12   \n",
      "10075  EEENF  2021-04-09 01:14:21+00:00       jerocker79   \n",
      "\n",
      "                     id_str  \\\n",
      "0      1375142994920271872a   \n",
      "1      1375143141058080768a   \n",
      "2      1375157484063584261a   \n",
      "3      1375176172099747845a   \n",
      "4      1375176361560604679a   \n",
      "...                     ...   \n",
      "10071  1380324815689539585a   \n",
      "10072  1380326289735708672a   \n",
      "10073  1380326328524623873a   \n",
      "10074  1380326575414120448a   \n",
      "10075  1380328195778506753a   \n",
      "\n",
      "                                                    text  Sentiment  \\\n",
      "0                        #rt  rt    axtg new intern ceo           0   \n",
      "1                    rt  #rt  rt    axtg new intern ceo           0   \n",
      "2                                   axtg big otc  gainer          1   \n",
      "3      todays top penny stock gainers\\n\\nlttgf axtg s...          1   \n",
      "4      rt  todays top penny stock gainers\\n\\nlttgf ax...          1   \n",
      "...                                                  ...        ...   \n",
      "10071  holding until at least summer wdhr eeenf phil ...          1   \n",
      "10072   yep well now that the bushy ass side burned d...         -1   \n",
      "10073  vper  phil imtl abml uatg sfor ltnc intc eeenf...          1   \n",
      "10074                                         rt   eeenf          1   \n",
      "10075   why eeenf to hcmc ive been waiting months for...         -1   \n",
      "\n",
      "       Known_Pumper  Price_Region  Inflection_Point  user_pumper_probability  \\\n",
      "0                 0           1.0               NaN                 0.006720   \n",
      "1                 0           1.0               NaN                 0.006720   \n",
      "2                 0           1.0               NaN                 0.025278   \n",
      "3                 0           1.0               NaN                 0.025278   \n",
      "4                 0           1.0               NaN                 0.025278   \n",
      "...             ...           ...               ...                      ...   \n",
      "10071             0           0.0               NaN                 0.025278   \n",
      "10072             0           0.0               NaN                 0.025278   \n",
      "10073             0           0.0               NaN                 0.025278   \n",
      "10074             0           0.0               NaN                 0.025278   \n",
      "10075             0           0.0               NaN                 0.025278   \n",
      "\n",
      "       ...  tweet  update  vmhg  way  wdlf  week  winners  xrp  yen  youre  \n",
      "0      ...    0.0     0.0   0.0  0.0   0.0   0.0      0.0  0.0  0.0    0.0  \n",
      "1      ...    0.0     0.0   0.0  0.0   0.0   0.0      0.0  0.0  0.0    0.0  \n",
      "2      ...    0.0     0.0   0.0  0.0   0.0   0.0      0.0  0.0  0.0    0.0  \n",
      "3      ...    0.0     0.0   0.0  0.0   0.0   0.0      0.0  0.0  0.0    0.0  \n",
      "4      ...    0.0     0.0   0.0  0.0   0.0   0.0      0.0  0.0  0.0    0.0  \n",
      "...    ...    ...     ...   ...  ...   ...   ...      ...  ...  ...    ...  \n",
      "10071  ...    0.0     0.0   0.0  0.0   0.0   0.0      0.0  0.0  0.0    0.0  \n",
      "10072  ...    0.0     0.0   0.0  0.0   0.0   0.0      0.0  0.0  0.0    0.0  \n",
      "10073  ...    0.0     0.0   0.0  0.0   0.0   0.0      0.0  0.0  0.0    0.0  \n",
      "10074  ...    0.0     0.0   0.0  0.0   0.0   0.0      0.0  0.0  0.0    0.0  \n",
      "10075  ...    0.0     0.0   0.0  0.0   0.0   0.0      0.0  0.0  0.0    0.0  \n",
      "\n",
      "[10076 rows x 118 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re\n",
    "import emoji\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Preprocesses the text by removing emojis, symbols, URLs, mentions, and punctuation.\n",
    "    \"\"\"\n",
    "    if isinstance(text, str): #check if text is a string.\n",
    "        text = emoji.demojize(text)  # Replace emojis with text descriptions\n",
    "        text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)  # Remove URLs\n",
    "        text = re.sub(r'@\\w+', '', text)  # Remove mentions\n",
    "        text = re.sub(r'[^\\w\\s#]', '', text)  # Remove punctuation (except hashtags)\n",
    "        text = re.sub(r'[^a-zA-Z\\s#]', '', text) #remove symbols.\n",
    "        text = text.lower()\n",
    "        return text\n",
    "    else:\n",
    "        return \"\" #if it is not a string, return empty string.\n",
    "\n",
    "def create_tfidf_features(df, text_column='text', max_features=100):\n",
    "    \"\"\"\n",
    "    Creates TF-IDF features from the specified text column with improved preprocessing.\n",
    "    \"\"\"\n",
    "\n",
    "    # Apply preprocessing to the text column\n",
    "    df[text_column] = df[text_column].apply(preprocess_text)\n",
    "\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        max_features=max_features,\n",
    "        stop_words='english',\n",
    "        token_pattern=r'\\b\\w+\\b|\\B#\\w+\\b'  # Include hashtags\n",
    "    )\n",
    "    tfidf_matrix = vectorizer.fit_transform(df[text_column])\n",
    "    tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "    df = pd.concat([df.reset_index(drop=True), tfidf_df.reset_index(drop=True)], axis=1)\n",
    "\n",
    "    return df,vectorizer\n",
    "\n",
    "df_with_tfidf_features,vectorizer = create_tfidf_features(df.copy())\n",
    "print(df_with_tfidf_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving tfidf_vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model_training/models/tfidf_vectorizer.joblib']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "joblib.dump(vectorizer, 'model_training/models/tfidf_vectorizer.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding centrality "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mentioned handles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_twitter_handles_from_dataframe(df):\n",
    "  \"\"\"\n",
    "  Extracts Twitter handles from a pandas DataFrame, where usernames are in\n",
    "  the 'user.screen_name' column and tweet text is in the 'text' column.\n",
    "\n",
    "  Args:\n",
    "    df: The pandas DataFrame.\n",
    "\n",
    "  Returns:\n",
    "    A pandas DataFrame with a new column 'mentioned_handles' containing lists of\n",
    "    extracted Twitter handles.\n",
    "  \"\"\"\n",
    "\n",
    "  def extract_handles(text):\n",
    "    if isinstance(text, str): #handle nan cases\n",
    "      pattern = r\"@([a-zA-Z0-9_]+)\"\n",
    "      handles = re.findall(pattern, text)\n",
    "      if len(handles) == 0:\n",
    "        return None\n",
    "      return handles\n",
    "    else:\n",
    "      return None\n",
    "\n",
    "  df['mentioned_handles'] = df['text'].apply(extract_handles)\n",
    "  return df\n",
    "\n",
    "twitter_df_with_handles = extract_twitter_handles_from_dataframe(df_with_tfidf_features.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_df_with_handles.to_csv(\"data/scored_tweets_final_translated_with_TFIDF.csv\",index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
